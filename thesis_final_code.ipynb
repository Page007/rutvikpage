{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8eada2",
   "metadata": {},
   "source": [
    "## THESIS: MSc Economics\n",
    "*Quantitative Economics, EC 475* <br>\n",
    "*The London School of Economics and Political Science* <br>\n",
    "*Thesis Advisor: Prof. Xavier Jaravel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "585c4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['axes.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b251d2",
   "metadata": {},
   "source": [
    "## Leave One Out Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6092b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_val(loss_1, loss_2, RSS_accumulator, algo1_result, algo2_result):\n",
    "    # Printing the RSS accumulator,\n",
    "    #print(RSS_accumulator)\n",
    "\n",
    "    # Now, the following is the algorithm we need to implement: \n",
    "    # I drop the chosen model\n",
    "    # I find the difference between the RSS for original model and all the other models.\n",
    "    # This gives me the loss suffered due to not choosing those other models\n",
    "    # Find the percentage of such losses that are more extreme than the chosen model;\n",
    "    # This gives me the p-value of the model\n",
    "\n",
    "    #print(algo1_result)\n",
    "    #print(algo2_result)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "\n",
    "    losses_1 = {}\n",
    "    losses_2 = {}\n",
    "    for i in RSS_accumulator: \n",
    "        if i != algo1_result:\n",
    "            losses_1[i] = RSS_accumulator[i] - RSS_accumulator['Original']\n",
    "            if losses_1[i] > loss_1:\n",
    "                count1 += 1\n",
    "        if i != algo2_result:\n",
    "            losses_2[i] = RSS_accumulator[i] - RSS_accumulator['Original']\n",
    "            if losses_2[i] > loss_2:\n",
    "                count2 += 1\n",
    "    # returning the p-values\n",
    "    return (count1/4, count2/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d4a83",
   "metadata": {},
   "source": [
    "## Data Generation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6869a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of observations; eventually, make this a loop\n",
    "def synthetic_data_generator(n = 100):\n",
    "\n",
    "    # Not setting random seed by design; we would like to keep the regressors stochastic, they might change their\n",
    "    # performance across the state space.\n",
    "\n",
    "    # rather than drawing from a uniformly random distribution, I uniformly randomly choose a distribution\n",
    "    # out of several different distributions and then generate regressors based on these \n",
    "\n",
    "    # all the regressors are either chosen at random from a uniform distribution or a Gaussian distribution; \n",
    "\n",
    "    coefficients = np.random.default_rng().normal(loc = np.random.randint(low = 0, high = 20, size = 1), scale = 20, size = 4)/10\n",
    "    choice_of_distribution = np.random.randint(2)\n",
    "    if choice_of_distribution == 0:\n",
    "        #print('Choosing data uniformly at random.. ')\n",
    "        data = np.random.rand(n, 4)\n",
    "    elif choice_of_distribution == 1:\n",
    "        #print('Choosing data from a Gaussian Distribution.. ')\n",
    "        data = np.random.default_rng().normal(loc = 0, scale = 1, size = (n, 4))\n",
    "\n",
    "    coefficients = np.transpose(coefficients)\n",
    "    data = pd.DataFrame(data, columns = ['x1', 'x2', 'x3', 'x4'])\n",
    "\n",
    "    # intermediate step for creating the outcome variable\n",
    "\n",
    "    intermediate = data.copy(deep = True)\n",
    "    intermediate['x1_square'] = intermediate['x1']*intermediate['x1']\n",
    "    intermediate['x2'] = np.log(intermediate['x2'])\n",
    "    intermediate['cross'] = intermediate['x2']*intermediate['x3']\n",
    "    intermediate = intermediate.drop(columns = ['x2', 'x3'])\n",
    "\n",
    "    # changing the columns of the pandas dataframe to what I want in the true model for the outcome variable\n",
    "    intermediate = np.matmul(intermediate, coefficients)\n",
    "\n",
    "    # adding error to the outcome I created \n",
    "    intermediate = pd.DataFrame(np.random.default_rng().normal(loc = 0, scale = 1, size = (n, )) + intermediate, columns = ['y'])\n",
    "    #intermediate = pd.DataFrame(intermediate, columns = ['y'])\n",
    "    data.insert(0, 'y', intermediate)\n",
    "    data = data.dropna()\n",
    "\n",
    "    #print(data)\n",
    "    del intermediate \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f73f9",
   "metadata": {},
   "source": [
    "## Starting Exercise: MinMax Principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27ae0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the minimiax function\n",
    "# this function takes as input the residual sums of squares (the solution to this stylised optimisation problem) \n",
    "# and returns the solution that is chosen by the minimax principle\n",
    "def miniMax(RSS_accumulator):\n",
    "    return max(RSS_accumulator, key = RSS_accumulator.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28258b1f",
   "metadata": {},
   "source": [
    "## Main Exercise: MiniMax Regret\n",
    "The main focus of this algorithm is to choose that model which has the minimum maximum loss. In our settings, the loss is defined by the difference between the minimum RSS between the incorrect model and the correct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e65911de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the minimax regret function; functionally speaking, find the regret first, then span the space\n",
    "# of the regressors and get the model with the least regret across all models.\n",
    "\n",
    "# if model A has RSS n1 and model B has RSS n2, where model B is the more correct representation of the world,\n",
    "# the regret, if model A is chosen is given by: n1 - n2 (the system suffers a loss of these many RSS units)\n",
    "def regret(n1, n2):\n",
    "    return n1 - n2\n",
    "\n",
    "def minimax_regret(RSS_accumulator):\n",
    "    max_regrets = list()\n",
    "    for i in range(1, len(RSS_accumulator) + 1):\n",
    "        temp = list()\n",
    "        for j in range(1, len(RSS_accumulator) + 1):\n",
    "            if i != j:\n",
    "                temp.append(regret(RSS_accumulator['result ' + str(i)], RSS_accumulator['result ' + str(j)]))\n",
    "        max_regrets.append(max(temp))\n",
    "    #print(max_regrets)\n",
    "    return 'result ' + str(min(range(len(max_regrets)), key=max_regrets.__getitem__) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a9896",
   "metadata": {},
   "source": [
    " _In_ this section of the code, we start claiming a model to be \n",
    " a 'true' model, offer a variety of alternatives that might seem plausible based on economic theory\n",
    " and see which one of these models is chosen by our different Decision theoretic algorithms.\n",
    " Let the true model in this case be: <br>\n",
    " $y_i = \\alpha + \\beta_1 * x_{1i} + \\beta_2 * ln(x_{1i}^2) + \\beta_3 * ln(x_{2i})*x_{3i} + \\beta_4 * x_{4i} + \\varepsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c00a1c7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def combinator(x_val = 3000):\n",
    "    loss_1 = list()\n",
    "    loss_2 = list()\n",
    "    index = 0\n",
    "    p_1 = 0\n",
    "    p_2 = 0\n",
    "    for i in range(100, x_val, 100):\n",
    "        data = synthetic_data_generator()\n",
    "        outcome = data['y']\n",
    "        RSS_accumulator = dict()\n",
    "\n",
    "        intermediate = data.copy(deep = True)\n",
    "        intermediate['x1_square'] = intermediate['x1']*intermediate['x1']\n",
    "        intermediate['x2'] = np.log(intermediate['x2'])\n",
    "        intermediate['cross'] = intermediate['x2']*intermediate['x3']\n",
    "        intermediate = intermediate.drop(columns = ['x2', 'x3'])\n",
    "        intermediate = sm.add_constant(intermediate)\n",
    "        intermediate = intermediate.drop(columns = ['y'])\n",
    "        original = sm.OLS(outcome, intermediate).fit()\n",
    "        RSS_accumulator['Original'] = original.ssr\n",
    "\n",
    "\n",
    "        # HYPOTHESISED MODEL I:(fitting a normal line in the four regressors)\n",
    "\n",
    "        regressors = pd.DataFrame()\n",
    "        for i in range(1, 5):\n",
    "            regressors['x' + str(i)] = data['x' + str(i)]\n",
    "        regressors = sm.add_constant(regressors)\n",
    "        model1 = sm.OLS(outcome, regressors)\n",
    "        result1 = model1.fit()\n",
    "        RSS_accumulator['result 1'] = result1.ssr\n",
    "\n",
    "        # HYPOTHESISED MODEL II:\n",
    "        # another model, simple.\n",
    "        # y_i = alpha + b1*x1_i + x2*x3*b2 + epsilon_i\n",
    "        regressors['x2'] = regressors['x2']*regressors['x3']\n",
    "        regressors = regressors.drop(columns = ['x3'])\n",
    "\n",
    "\n",
    "        result2 = sm.OLS(outcome, regressors).fit()\n",
    "        RSS_accumulator['result 2']= result2.ssr\n",
    "\n",
    "        # HYPOTHESISED MODEL III:\n",
    "        # another simple model.\n",
    "        # y_i = alpha + b1*x1*x2 + b2*x2*x3 + b4*x2*x4 + epsilon_i\n",
    "\n",
    "        del(regressors)\n",
    "        regressors = pd.DataFrame()\n",
    "        for i in range(1, 5):\n",
    "            regressors['x' + str(i)] = data['x' + str(i)]\n",
    "        regressors = sm.add_constant(regressors)\n",
    "        regressors['x1'] = regressors['x1']*regressors['x2']\n",
    "        regressors['x3'] = regressors['x3']*regressors['x2']\n",
    "        regressors['x4'] = regressors['x4']*regressors['x2']\n",
    "        regressors = regressors.drop(columns = ['x2'])\n",
    "\n",
    "        result3 = sm.OLS(outcome, regressors).fit()\n",
    "        RSS_accumulator['result 3']= result3.ssr\n",
    "\n",
    "        # HYPOTHESISED MODEL IV: choosing all the second order interaction terms\n",
    "        list1 = list(combinations([i for i in range(1, 5)], 2))\n",
    "\n",
    "        # Now generating a new list of regressors,\n",
    "\n",
    "        regressors = pd.DataFrame()\n",
    "        for i in range(len(list1)):\n",
    "            regressors['x' + str(i)] = data['x' + str(list1[i][0])]*data['x' + str(list1[i][1])]\n",
    "        for i in range(5, 9):\n",
    "            regressors['x' + str(i)] = data['x' + str(i - 4)]\n",
    "\n",
    "        # Running OLS of the regressors on the outcome\n",
    "        result4 = sm.OLS(outcome, regressors).fit()\n",
    "        RSS_accumulator['result 4']= result4.ssr\n",
    "\n",
    "        # HYPOTHESISED MODEL V: choosing all the first, second and third order terms\n",
    "        list1 = list(combinations([i for i in range(1, 5)], 3))\n",
    "        for i in range(9, 9 + len(list1)):\n",
    "            regressors['x' + str(i)] = data['x' + str(list1[i-9][0])]*data['x' + str(list1[i-9][1])]*data['x' + str(list1[i-9][2])]\n",
    "\n",
    "        result5 = sm.OLS(outcome, regressors).fit()\n",
    "        #RSS_accumulator['result 5'] = result5.ssr\n",
    "\n",
    "        #output_object = summary_col([original, result1, result2, result3], stars = True)\n",
    "        #print(output_object)\n",
    "\n",
    "        # HYPOTHESISED MODEL VI: Cox proportional hazard model\n",
    "\n",
    "\n",
    "        # Note that across all these regressions, the values that the mentioned regressors take are different; for the purposes of \n",
    "        # brevity, they have not been mentioned in full. \n",
    "        #print(results.params)\n",
    "        #print('The residual sum of squares for this model is: ', results.ssr)\n",
    "        #print(result.summary(slim = True).as_latex())\n",
    "        #for i in RSS_accumulator:\n",
    "        #    print(i, RSS_accumulator[i])\n",
    "        solution_1 = miniMax(RSS_accumulator)\n",
    "\n",
    "\n",
    "        test = RSS_accumulator.copy()\n",
    "        del test['Original']\n",
    "\n",
    "        algo1_result = miniMax(test)\n",
    "        algo2_result = minimax_regret(test)\n",
    "        #print(algo1_result)\n",
    "        #print(algo2_result)\n",
    "\n",
    "\n",
    "        loss_1.append(RSS_accumulator[algo1_result] - RSS_accumulator['Original'])\n",
    "        loss_2.append(RSS_accumulator[algo2_result] - RSS_accumulator['Original'])\n",
    "        performance = p_val(loss_1[index], loss_2[index], RSS_accumulator, algo1_result, algo2_result)\n",
    "        #print('Performance for algo. 1:', performance[0])\n",
    "        #print('Performance for algo. 2:', performance[1])\n",
    "        p_1 += performance[0]\n",
    "        p_2 += performance[1]\n",
    "        index += 1\n",
    "    return (loss_1, loss_2, p_1/50, p_2/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2f4e8",
   "metadata": {},
   "source": [
    "## Data Plotting and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19551767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.76\n",
      "0.0 0.77\n",
      "0.0 0.77\n",
      "0.0 0.755\n"
     ]
    }
   ],
   "source": [
    "# I could have done the following exercise with a loop, but I am bored right now.\n",
    "\n",
    "figure, axis = plt.subplots(nrows = 2, ncols = 2, layout = 'constrained')\n",
    "x_axis = [i for i in range(100, 5000, 100)]\n",
    "loss_1, loss_2, a1, a2 = combinator(5000)\n",
    "p1,=axis[0, 0].plot(x_axis, loss_1)\n",
    "axis[0, 0].set_title('Losses in Algorithms')\n",
    "axis[0, 0].set_xlabel('Sample Size')\n",
    "axis[0, 0].set_ylabel('Loss')\n",
    "axis[0, 0].plot(x_axis, loss_2)\n",
    "print(a1, a2)\n",
    "#plt.show()\n",
    "\n",
    "loss_1, loss_2, a1, a2  = combinator(5000)\n",
    "p2,=axis[0, 1].plot(x_axis, loss_1)\n",
    "axis[0, 1].set_title('Losses in Algorithms')\n",
    "axis[0, 1].set_xlabel('Sample Size')\n",
    "axis[0, 1].set_ylabel('Loss')\n",
    "axis[0, 1].plot(x_axis, loss_2)\n",
    "print(a1, a2)\n",
    "\n",
    "loss_1, loss_2, a1, a2 = combinator(5000)\n",
    "p3,=axis[1, 0].plot(x_axis, loss_1)\n",
    "axis[1, 0].set_title('Losses in Algorithms')\n",
    "axis[1, 0].set_xlabel('Sample Size')\n",
    "axis[1, 0].set_ylabel('Loss')\n",
    "axis[1, 0].plot(x_axis, loss_2)\n",
    "print(a1, a2)\n",
    "\n",
    "loss_1, loss_2, a1, a2 = combinator(5000)\n",
    "p4,=axis[1, 1].plot(x_axis, loss_1)\n",
    "axis[1, 1].set_title('Losses in Algorithms')\n",
    "axis[1, 1].set_xlabel('Sample Size')\n",
    "axis[1, 1].set_ylabel('Loss')\n",
    "p5,=axis[1, 1].plot(x_axis, loss_2)\n",
    "print(a1, a2)\n",
    "\n",
    "axis[1, 1].legend((p4, p5), ('Algo 1', 'Algo 2'), loc = 'upper right', shadow = True)\n",
    "figure.savefig('graph_performance.png')\n",
    "plt.close(figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
